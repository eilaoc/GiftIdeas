import os
import pandas as pd
from tqdm import tqdm
import kagglehub
import json
import spacy

# === CONFIGURATION ===
CACHE_FILE = "simplify_cache.json"
OUTPUT_CSV = "generalized_gift_ideas.csv"
INR_TO_GBP = 0.0085

# === Setup spaCy ===
print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm")

tqdm.pandas()

# === Load cache if exists ===
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        simplify_cache = json.load(f)
else:
    simplify_cache = {}

# === Load dataset ===
print("ðŸ“¦ Downloading/loading dataset from Kaggle...")
path = kagglehub.dataset_download("lokeshparab/amazon-products-dataset")

print("ðŸ“‚ Loading CSV files...")
all_files = [f for f in os.listdir(path) if f.endswith('.csv')]
dataframes = []
for file in all_files:
    try:
        df = pd.read_csv(os.path.join(path, file))
        dataframes.append(df)
        print(f"âœ… Loaded: {file}")
    except Exception as e:
        print(f"âš ï¸ Could not load {file}: {e}")

df = pd.concat(dataframes, ignore_index=True)

needed_cols = ['name', 'main_category', 'sub_category', 'ratings', 'actual_price']
df = df[[c for c in needed_cols if c in df.columns]]

print(f"\nðŸ§¾ Total products loaded: {len(df)}")
print("ðŸ§ª Sample:", df['name'].iloc[0])

# === Reduce dataset size by stratified sampling on 'main_category' ===
sample_per_group = 1000  # Adjust this number based on desired dataset size

sampled_dfs = []
for group, group_df in df.groupby('main_category'):
    sampled = group_df.sample(n=min(sample_per_group, len(group_df)), random_state=42)
    sampled_dfs.append(sampled)

df = pd.concat(sampled_dfs).reset_index(drop=True)

print(f"\nðŸ§¾ Total products after sampling: {len(df)}")



# === Simplify function using spaCy noun chunks ===
def generalize_product_name(name):
    if name in simplify_cache:
        return simplify_cache[name]

    doc = nlp(name)
    # Extract noun chunks, choose the longest one (heuristic for "main product idea")
    noun_chunks = [chunk.text.strip() for chunk in doc.noun_chunks if len(chunk.text.strip()) > 1]
    if noun_chunks:
        # Pick the longest noun chunk as simplified idea
        simplified = max(noun_chunks, key=len)
    else:
        # fallback: take first 2 words
        simplified = ' '.join(name.split()[:2])

    simplify_cache[name] = simplified
    return simplified

# === Run generalization on dataset ===
print("\nðŸ§  Generating gift ideas...")
df['gift_idea'] = df['name'].progress_apply(generalize_product_name)

# === Save cache ===
with open(CACHE_FILE, "w", encoding="utf-8") as f:
    json.dump(simplify_cache, f, ensure_ascii=False, indent=2)

# === Convert INR â†’ GBP ===
def inr_to_gbp(value):
    try:
        value = str(value).replace(',', '').replace('â‚¹', '').strip()
        return round(float(value) * INR_TO_GBP, 2)
    except:
        return None

df['price_gbp'] = df['actual_price'].apply(inr_to_gbp)
df['ratings'] = pd.to_numeric(df['ratings'], errors='coerce')

# === Group by gift idea ===
grouped = df.groupby('gift_idea').agg({
    'price_gbp': 'mean',
    'ratings': 'mean',
    'main_category': lambda x: list(set(x.dropna())),
    'sub_category': lambda x: list(set(x.dropna())),
    'name': 'count'
}).reset_index()

grouped.rename(columns={
    'price_gbp': 'average_price_gbp',
    'ratings': 'average_rating',
    'main_category': 'main_categories',
    'sub_category': 'sub_categories',
    'name': 'num_products'
}, inplace=True)

grouped = grouped[grouped['gift_idea'].notnull()]
grouped = grouped[grouped['gift_idea'] != "Unknown"]

grouped.to_csv(OUTPUT_CSV, index=False)
print(f"\nâœ… Gift ideas saved to: {OUTPUT_CSV}")
